# Fly.io DockeréŸ³å£°å‡¦ç†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

## æ¦‚è¦

éŸ³å£°å‡¦ç†ã‚’Fly.ioä¸Šã®Dockerã‚³ãƒ³ãƒ†ãƒŠã§å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€Supabase Edge Functionsã®åˆ¶ç´„ï¼ˆ2ç§’CPUåˆ¶é™ã€150MBãƒ¡ãƒ¢ãƒªåˆ¶é™ï¼‰ã‚’å›é¿ã—ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ã§å®‰å®šã—ãŸéŸ³å£°å‡¦ç†ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³

```
[Client] â”€â”€Uploadâ”€â”€> [Fly.io S3 Storage]
    â”‚                        â”‚
    â”‚ WebSocket              â”‚ Trigger
    â–¼                        â–¼
[Supabase Realtime] <â”€â”€ [Fly.io Docker Container]
    â”‚                        â”‚
    â”‚                        â”œâ”€â”€ [Whisper API]
    â”‚                        â”œâ”€â”€ [Gemini API]
    â”‚                        â””â”€â”€ [ffmpeg/Audio Tools]
    â”‚                        â”‚
    â”‚                        â–¼
    â””â”€â”€ Progress â”€â”€â”€â”€ [Supabase Database]
```

## 1. Fly.io Dockeræ§‹æˆ

### 1.1 Dockerfileè¨­è¨ˆ

```dockerfile
# Dockerfile
FROM python:3.11-slim

# ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
RUN apt-get update && apt-get install -y \
    ffmpeg \
    curl \
    && rm -rf /var/lib/apt/lists/*

# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
WORKDIR /app

# Pythonä¾å­˜é–¢ä¿‚
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ‰
COPY . .

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# ãƒãƒ¼ãƒˆå…¬é–‹
EXPOSE 8080

# å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰
CMD ["python", "app.py"]
```

### 1.2 requirements.txt

```txt
# requirements.txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
httpx==0.25.2
python-multipart==0.0.6
boto3==1.34.0
supabase==2.0.0
openai==1.3.0
google-generativeai==0.3.0
pydub==0.25.1
numpy==1.24.3
python-dotenv==1.0.0
redis==5.0.1
celery==5.3.4
```

### 1.3 fly.tomlè¨­å®š

```toml
# fly.toml
app = "vibeinsight-audio-processor"
primary_region = "nrt"

[build]
  dockerfile = "Dockerfile"

[env]
  PORT = "8080"
  ENVIRONMENT = "production"

[http_service]
  internal_port = 8080
  force_https = true
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 0
  processes = ["app"]

[[http_service.checks]]
  grace_period = "10s"
  interval = "30s"
  method = "GET"
  timeout = "5s"
  path = "/health"

[vm]
  cpu_kind = "shared"
  cpus = 2
  memory_mb = 2048

[metrics]
  port = 9091
  path = "/metrics"

[[services]]
  internal_port = 8080
  protocol = "tcp"
  
  [[services.ports]]
    handlers = ["http"]
    port = 80
    force_https = true
  
  [[services.ports]]
    handlers = ["tls", "http"]
    port = 443
  
  [[services.tcp_checks]]
    grace_period = "10s"
    interval = "30s"
    timeout = "5s"

[volumes]
  # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒœãƒªãƒ¥ãƒ¼ãƒ 
  [[volumes.mounts]]
    source = "audio_temp"
    destination = "/tmp/audio"
    processes = ["app"]

[secrets]
  # ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦è¨­å®š
  OPENAI_API_KEY = ""
  GOOGLE_API_KEY = ""
  SUPABASE_URL = ""
  SUPABASE_SERVICE_KEY = ""
  FLY_S3_ACCESS_KEY_ID = ""
  FLY_S3_SECRET_ACCESS_KEY = ""
  FLY_S3_ENDPOINT = ""
  REDIS_URL = ""
```

## 2. éŸ³å£°å‡¦ç†ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

### 2.1 FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

```python
# app.py
from fastapi import FastAPI, BackgroundTasks, HTTPException
from pydantic import BaseModel
import asyncio
import httpx
import os
from audio_processor import AudioProcessor
from supabase_client import SupabaseClient
from s3_client import S3Client

app = FastAPI(title="VibeInsight Audio Processor", version="1.0.0")

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
audio_processor = AudioProcessor()
supabase_client = SupabaseClient()
s3_client = S3Client()

class ProcessingRequest(BaseModel):
    session_id: str
    audio_url: str
    user_id: str

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "status": "healthy",
        "version": "1.0.0",
        "timestamp": datetime.utcnow().isoformat()
    }

@app.post("/process-audio")
async def process_audio(
    request: ProcessingRequest,
    background_tasks: BackgroundTasks
):
    """éŸ³å£°å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§éŸ³å£°å‡¦ç†ã‚’é–‹å§‹
        background_tasks.add_task(
            process_audio_background,
            request.session_id,
            request.audio_url,
            request.user_id
        )
        
        return {
            "message": "Processing started",
            "session_id": request.session_id,
            "status": "processing"
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

async def process_audio_background(session_id: str, audio_url: str, user_id: str):
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰éŸ³å£°å‡¦ç†"""
    try:
        # é€²æ—æ›´æ–°: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹
        await supabase_client.update_progress(
            session_id, "downloading", 10, "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."
        )
        
        # S3ã‹ã‚‰éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        audio_file = await s3_client.download_file(audio_url)
        
        # é€²æ—æ›´æ–°: å‰å‡¦ç†
        await supabase_client.update_progress(
            session_id, "preprocessing", 20, "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰å‡¦ç†ä¸­..."
        )
        
        # éŸ³å£°å‰å‡¦ç†ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ã€ãƒã‚¤ã‚ºé™¤å»ç­‰ï¼‰
        processed_audio = await audio_processor.preprocess_audio(audio_file)
        
        # é€²æ—æ›´æ–°: æ–‡å­—èµ·ã“ã—
        await supabase_client.update_progress(
            session_id, "transcribing", 40, "éŸ³å£°ã‚’æ–‡å­—ã«å¤‰æ›ä¸­..."
        )
        
        # Whisper APIã§æ–‡å­—èµ·ã“ã—
        transcript_result = await audio_processor.transcribe_with_whisper(
            processed_audio, session_id
        )
        
        # é€²æ—æ›´æ–°: è©±è€…åˆ†é›¢
        await supabase_client.update_progress(
            session_id, "diarization", 60, "è©±è€…ã‚’è­˜åˆ¥ä¸­..."
        )
        
        # è©±è€…åˆ†é›¢å‡¦ç†
        diarized_transcript = await audio_processor.perform_diarization(
            processed_audio, transcript_result
        )
        
        # é€²æ—æ›´æ–°: AIåˆ†æ
        await supabase_client.update_progress(
            session_id, "analyzing", 80, "AIãŒä¼šè©±ã‚’åˆ†æä¸­..."
        )
        
        # Gemini APIã§åˆ†æ
        analysis_result = await audio_processor.analyze_with_gemini(
            diarized_transcript, session_id
        )
        
        # é€²æ—æ›´æ–°: çµæœä¿å­˜
        await supabase_client.update_progress(
            session_id, "saving", 95, "çµæœã‚’ä¿å­˜ä¸­..."
        )
        
        # çµæœã‚’Supabaseã«ä¿å­˜
        await supabase_client.save_analysis_results(
            session_id, diarized_transcript, analysis_result
        )
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        await cleanup_temp_files(audio_file, processed_audio)
        
        # é€²æ—æ›´æ–°: å®Œäº†
        await supabase_client.update_progress(
            session_id, "completed", 100, "åˆ†æå®Œäº†ï¼"
        )
        
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®é€²æ—æ›´æ–°
        await supabase_client.update_progress(
            session_id, "error", 0, f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        )
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°è¨˜éŒ²
        logger.error(f"Audio processing failed for session {session_id}: {str(e)}")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        await cleanup_temp_files(audio_file, processed_audio)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=int(os.getenv("PORT", 8080)),
        reload=False
    )
```

### 2.2 éŸ³å£°å‡¦ç†ã‚¯ãƒ©ã‚¹

```python
# audio_processor.py
import asyncio
import tempfile
import os
from pathlib import Path
from pydub import AudioSegment
from pydub.silence import split_on_silence
import openai
import google.generativeai as genai
import numpy as np
from typing import Dict, List, Tuple

class AudioProcessor:
    def __init__(self):
        # API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        openai.api_key = os.getenv("OPENAI_API_KEY")
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        
        # éŸ³å£°å‡¦ç†è¨­å®š
        self.target_sample_rate = 16000
        self.max_whisper_size = 25 * 1024 * 1024  # 25MB
        self.chunk_duration = 20 * 60  # 20åˆ†
    
    async def preprocess_audio(self, audio_file_path: str) -> str:
        """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰å‡¦ç†"""
        try:
            # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            audio = AudioSegment.from_file(audio_file_path)
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆæ­£è¦åŒ–
            if audio.frame_rate != self.target_sample_rate:
                audio = audio.set_frame_rate(self.target_sample_rate)
            
            # ãƒ¢ãƒãƒ©ãƒ«å¤‰æ›
            if audio.channels > 1:
                audio = audio.set_channels(1)
            
            # éŸ³é‡æ­£è¦åŒ–
            audio = self.normalize_audio(audio)
            
            # ãƒã‚¤ã‚ºé™¤å»ï¼ˆåŸºæœ¬çš„ãªã‚‚ã®ï¼‰
            audio = self.reduce_noise(audio)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            temp_file = tempfile.NamedTemporaryFile(
                suffix=".wav", delete=False, dir="/tmp/audio"
            )
            audio.export(temp_file.name, format="wav")
            
            return temp_file.name
            
        except Exception as e:
            raise Exception(f"Audio preprocessing failed: {str(e)}")
    
    def normalize_audio(self, audio: AudioSegment) -> AudioSegment:
        """éŸ³é‡æ­£è¦åŒ–"""
        # å¹³å‡éŸ³é‡ã‚’-20dBFSã«æ­£è¦åŒ–
        change_in_dBFS = -20.0 - audio.dBFS
        return audio.apply_gain(change_in_dBFS)
    
    def reduce_noise(self, audio: AudioSegment) -> AudioSegment:
        """åŸºæœ¬çš„ãªãƒã‚¤ã‚ºé™¤å»"""
        # ç„¡éŸ³éƒ¨åˆ†ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°
        return audio.strip_silence(silence_len=500, silence_thresh=-50)
    
    async def transcribe_with_whisper(self, audio_file_path: str, session_id: str) -> Dict:
        """Whisper APIã§æ–‡å­—èµ·ã“ã—"""
        try:
            file_size = os.path.getsize(audio_file_path)
            
            if file_size > self.max_whisper_size:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„å ´åˆã¯åˆ†å‰²å‡¦ç†
                return await self.transcribe_large_file(audio_file_path, session_id)
            
            # é€šå¸¸ã®å‡¦ç†
            with open(audio_file_path, "rb") as audio_file:
                response = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: openai.Audio.transcribe(
                        "whisper-1",
                        audio_file,
                        response_format="verbose_json",
                        language="ja"
                    )
                )
            
            return response
            
        except Exception as e:
            raise Exception(f"Whisper transcription failed: {str(e)}")
    
    async def transcribe_large_file(self, audio_file_path: str, session_id: str) -> Dict:
        """å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†å‰²å‡¦ç†"""
        try:
            # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            audio = AudioSegment.from_file(audio_file_path)
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunk_length_ms = self.chunk_duration * 1000
            chunks = [
                audio[i:i + chunk_length_ms]
                for i in range(0, len(audio), chunk_length_ms)
            ]
            
            # å„ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†
            tasks = []
            for i, chunk in enumerate(chunks):
                chunk_file = f"/tmp/audio/chunk_{session_id}_{i}.wav"
                chunk.export(chunk_file, format="wav")
                
                task = self.transcribe_chunk(chunk_file, i)
                tasks.append(task)
            
            # ä¸¦åˆ—å®Ÿè¡Œ
            chunk_results = await asyncio.gather(*tasks)
            
            # çµæœã‚’ãƒãƒ¼ã‚¸
            merged_result = self.merge_transcription_results(chunk_results)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            for i in range(len(chunks)):
                chunk_file = f"/tmp/audio/chunk_{session_id}_{i}.wav"
                if os.path.exists(chunk_file):
                    os.remove(chunk_file)
            
            return merged_result
            
        except Exception as e:
            raise Exception(f"Large file transcription failed: {str(e)}")
    
    async def transcribe_chunk(self, chunk_file: str, chunk_index: int) -> Dict:
        """å€‹åˆ¥ãƒãƒ£ãƒ³ã‚¯ã®æ–‡å­—èµ·ã“ã—"""
        with open(chunk_file, "rb") as audio_file:
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: openai.Audio.transcribe(
                    "whisper-1",
                    audio_file,
                    response_format="verbose_json",
                    language="ja"
                )
            )
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
        response["chunk_index"] = chunk_index
        return response
    
    def merge_transcription_results(self, chunk_results: List[Dict]) -> Dict:
        """ãƒãƒ£ãƒ³ã‚¯çµæœã®ãƒãƒ¼ã‚¸"""
        # ãƒãƒ£ãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«ã‚½ãƒ¼ãƒˆ
        chunk_results.sort(key=lambda x: x["chunk_index"])
        
        merged_segments = []
        time_offset = 0.0
        
        for chunk_result in chunk_results:
            for segment in chunk_result.get("segments", []):
                # æ™‚é–“ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’èª¿æ•´
                segment["start"] += time_offset
                segment["end"] += time_offset
                merged_segments.append(segment)
            
            # æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¨ˆç®—
            if chunk_result.get("segments"):
                time_offset = chunk_result["segments"][-1]["end"]
        
        # ãƒãƒ¼ã‚¸ã•ã‚ŒãŸçµæœã‚’æ§‹ç¯‰
        merged_text = " ".join([segment["text"] for segment in merged_segments])
        
        return {
            "text": merged_text,
            "segments": merged_segments,
            "language": "ja"
        }
    
    async def perform_diarization(self, audio_file_path: str, transcript: Dict) -> Dict:
        """è©±è€…åˆ†é›¢å‡¦ç†"""
        try:
            # pyannote.audioã‚„speechbrainã‚’ä½¿ç”¨ã—ãŸè©±è€…åˆ†é›¢
            # ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã—ãŸå®Ÿè£…ä¾‹
            
            # éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“è©±è€…åˆ†é›¢
            audio = AudioSegment.from_file(audio_file_path)
            segments = transcript.get("segments", [])
            
            # éŸ³é‡ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“è©±è€…åˆ†é›¢
            diarized_segments = []
            for segment in segments:
                start_ms = int(segment["start"] * 1000)
                end_ms = int(segment["end"] * 1000)
                
                # è©²å½“éƒ¨åˆ†ã®éŸ³å£°ã‚’å–å¾—
                segment_audio = audio[start_ms:end_ms]
                
                # éŸ³é‡ã§è©±è€…ã‚’æ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
                if segment_audio.dBFS > -25:
                    speaker = "coach"  # ã‚ˆã‚Šå¤§ããªå£°
                else:
                    speaker = "client"  # ã‚ˆã‚Šå°ã•ãªå£°
                
                diarized_segments.append({
                    **segment,
                    "speaker": speaker
                })
            
            return {
                "text": transcript["text"],
                "segments": diarized_segments,
                "speakers": ["coach", "client"]
            }
            
        except Exception as e:
            # è©±è€…åˆ†é›¢ã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè©±è€…ã‚’è¨­å®š
            segments = transcript.get("segments", [])
            for segment in segments:
                segment["speaker"] = "unknown"
            
            return {
                "text": transcript["text"],
                "segments": segments,
                "speakers": ["unknown"]
            }
    
    async def analyze_with_gemini(self, transcript_data: Dict, session_id: str) -> Dict:
        """Gemini APIã§åˆ†æ"""
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            prompt = self.build_analysis_prompt(transcript_data)
            
            # Gemini APIå‘¼ã³å‡ºã—
            model = genai.GenerativeModel("gemini-1.5-flash")
            
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: model.generate_content(prompt)
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’JSONã«å¤‰æ›
            analysis_result = self.parse_gemini_response(response.text)
            
            return analysis_result
            
        except Exception as e:
            raise Exception(f"Gemini analysis failed: {str(e)}")
    
    def build_analysis_prompt(self, transcript_data: Dict) -> str:
        """åˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰"""
        segments = transcript_data.get("segments", [])
        
        # è©±è€…åˆ¥ã®ç™ºè¨€ã‚’æ•´ç†
        conversation = []
        for segment in segments:
            speaker = segment.get("speaker", "unknown")
            text = segment.get("text", "")
            start_time = segment.get("start", 0)
            
            conversation.append(f"[{start_time:.1f}s] {speaker}: {text}")
        
        conversation_text = "\n".join(conversation)
        
        prompt = f"""
ä»¥ä¸‹ã¯1on1ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®ä¼šè©±è¨˜éŒ²ã§ã™ã€‚ã‚³ãƒ¼ãƒãƒ³ã‚°åˆ†æã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

ä¼šè©±è¨˜éŒ²:
{conversation_text}

ä»¥ä¸‹ã®å½¢å¼ã§JSONã§å›ç­”ã—ã¦ãã ã•ã„:
{{
  "speaker_analysis": {{
    "coach_speaking_ratio": 0.6,
    "client_speaking_ratio": 0.4,
    "gini_coefficient": 0.2
  }},
  "coaching_skills": {{
    "listening": {{ "score": 4, "examples": ["å…·ä½“çš„ãªç™ºè¨€ä¾‹"] }},
    "questioning": {{ "score": 3, "examples": ["è³ªå•ã®ä¾‹"] }},
    "empathy": {{ "score": 5, "examples": ["å…±æ„Ÿã®ä¾‹"] }},
    "feedback": {{ "score": 3, "examples": ["ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ä¾‹"] }}
  }},
  "emotion_analysis": {{
    "coach": {{ "positive": 60, "neutral": 35, "negative": 5 }},
    "client": {{ "positive": 40, "neutral": 50, "negative": 10 }}
  }},
  "conversation_events": [
    {{ "type": "overlap", "speaker": "coach", "time": 120, "description": "ç™ºè¨€ã®é‡è¤‡" }},
    {{ "type": "silence", "time": 180, "duration": 5, "description": "é•·ã„æ²ˆé»™" }}
  ],
  "diagnosis": {{
    "name": "ãƒã‚·ãƒ³ã‚¬ãƒ³ãƒ»ã‚½ãƒ«ãƒãƒ¼",
    "icon": "ğŸ—£ï¸",
    "description": "è§£æ±ºç­–ã‚’æ¬¡ã€…ã¨æç¤ºã™ã‚‹ã‚¿ã‚¤ãƒ—"
  }},
  "prescriptions": [
    "ç›¸æ‰‹ãŒè©±ã—çµ‚ãˆãŸå¾Œã€3ç§’å¾…ã£ã¦ã‹ã‚‰è©±ã—å§‹ã‚ã‚‹",
    "è³ªå•ã‚’ã—ãŸå¾Œã¯ã€ç›¸æ‰‹ã®å›ç­”ã‚’æœ€å¾Œã¾ã§èã"
  ]
}}
"""
        return prompt
    
    def parse_gemini_response(self, response_text: str) -> Dict:
        """Geminiãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ‘ãƒ¼ã‚¹"""
        try:
            # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
            import json
            import re
            
            # ```json ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
            json_match = re.search(r'```json\n(.*?)\n```', response_text, re.DOTALL)
            if json_match:
                json_text = json_match.group(1)
            else:
                # { } ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    json_text = json_match.group(0)
                else:
                    raise Exception("JSON format not found in response")
            
            return json.loads(json_text)
            
        except Exception as e:
            # ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
            return {
                "speaker_analysis": {
                    "coach_speaking_ratio": 0.5,
                    "client_speaking_ratio": 0.5,
                    "gini_coefficient": 0.0
                },
                "coaching_skills": {
                    "listening": {"score": 3, "examples": []},
                    "questioning": {"score": 3, "examples": []},
                    "empathy": {"score": 3, "examples": []},
                    "feedback": {"score": 3, "examples": []}
                },
                "emotion_analysis": {
                    "coach": {"positive": 50, "neutral": 40, "negative": 10},
                    "client": {"positive": 50, "neutral": 40, "negative": 10}
                },
                "conversation_events": [],
                "diagnosis": {
                    "name": "åˆ†æä¸­",
                    "icon": "ğŸ¤–",
                    "description": "åˆ†æå‡¦ç†ä¸­ã§ã™"
                },
                "prescriptions": ["åˆ†æçµæœã®æº–å‚™ä¸­ã§ã™"]
            }
```

## 3. ãƒ‡ãƒ—ãƒ­ã‚¤ã¨é‹ç”¨

### 3.1 ãƒ‡ãƒ—ãƒ­ã‚¤ã‚³ãƒãƒ³ãƒ‰

```bash
# Fly.ioã‚¢ãƒ—ãƒªä½œæˆ
fly apps create vibeinsight-audio-processor

# ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆè¨­å®š
fly secrets set OPENAI_API_KEY="your_openai_key"
fly secrets set GOOGLE_API_KEY="your_google_key"
fly secrets set SUPABASE_URL="your_supabase_url"
fly secrets set SUPABASE_SERVICE_KEY="your_service_key"
fly secrets set FLY_S3_ACCESS_KEY_ID="your_s3_key"
fly secrets set FLY_S3_SECRET_ACCESS_KEY="your_s3_secret"

# ãƒœãƒªãƒ¥ãƒ¼ãƒ ä½œæˆ
fly volumes create audio_temp --region nrt --size 10

# ãƒ‡ãƒ—ãƒ­ã‚¤
fly deploy

# ãƒ­ã‚°ç¢ºèª
fly logs

# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
fly scale count 2
fly scale memory 4096
```

### 3.2 ç›£è¦–è¨­å®š

```python
# monitoring.py
import time
import psutil
from prometheus_client import Counter, Histogram, Gauge
from fastapi import Request
import logging

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
processing_requests_total = Counter(
    'audio_processing_requests_total',
    'Total audio processing requests',
    ['status']
)

processing_duration_seconds = Histogram(
    'audio_processing_duration_seconds',
    'Audio processing duration in seconds',
    ['stage']
)

active_processing_jobs = Gauge(
    'active_processing_jobs',
    'Number of active processing jobs'
)

memory_usage_bytes = Gauge(
    'memory_usage_bytes',
    'Memory usage in bytes'
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ProcessingMonitor:
    def __init__(self):
        self.active_jobs = 0
    
    def start_job(self, session_id: str):
        self.active_jobs += 1
        active_processing_jobs.set(self.active_jobs)
        logger.info(f"Started processing job: {session_id}")
    
    def complete_job(self, session_id: str, status: str, duration: float):
        self.active_jobs -= 1
        active_processing_jobs.set(self.active_jobs)
        processing_requests_total.labels(status=status).inc()
        logger.info(f"Completed processing job: {session_id}, status: {status}, duration: {duration}s")
    
    def record_stage_duration(self, stage: str, duration: float):
        processing_duration_seconds.labels(stage=stage).observe(duration)
    
    def update_system_metrics(self):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ›´æ–°"""
        memory_info = psutil.virtual_memory()
        memory_usage_bytes.set(memory_info.used)

monitor = ProcessingMonitor()
```

## 4. Supabaseé€£æºã®æ›´æ–°

### 4.1 Edge Function (è»½é‡åŒ–)

```typescript
// supabase/functions/trigger-audio-processing/index.ts
import { serve } from 'https://deno.land/std@0.177.0/http/server.ts';
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';

serve(async (req) => {
  try {
    const { sessionId, audioUrl, userId } = await req.json();
    
    // Fly.io Dockerã‚³ãƒ³ãƒ†ãƒŠã«å‡¦ç†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
    const processingResponse = await fetch(
      `${Deno.env.get('FLY_AUDIO_PROCESSOR_URL')}/process-audio`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${Deno.env.get('FLY_AUTH_TOKEN')}`
        },
        body: JSON.stringify({
          session_id: sessionId,
          audio_url: audioUrl,
          user_id: userId
        })
      }
    );
    
    if (!processingResponse.ok) {
      throw new Error(`Processing request failed: ${processingResponse.statusText}`);
    }
    
    const result = await processingResponse.json();
    
    return new Response(JSON.stringify(result), {
      headers: { 'Content-Type': 'application/json' },
      status: 200
    });
    
  } catch (error) {
    return new Response(JSON.stringify({ error: error.message }), {
      headers: { 'Content-Type': 'application/json' },
      status: 500
    });
  }
});
```

## 5. ãƒ¡ãƒªãƒƒãƒˆã¨åˆ¶ç´„è§£æ±º

### 5.1 è§£æ±ºã•ã‚Œã‚‹åˆ¶ç´„

| åˆ¶ç´„ | Supabase Edge Functions | Fly.io Docker |
|------|-------------------------|---------------|
| **CPUæ™‚é–“** | 2ç§’åˆ¶é™ | âœ… ç„¡åˆ¶é™ |
| **ãƒ¡ãƒ¢ãƒª** | 150MBåˆ¶é™ | âœ… æœ€å¤§8GB |
| **å‡¦ç†æ™‚é–“** | ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒªã‚¹ã‚¯ | âœ… é•·æ™‚é–“å‡¦ç†å¯¾å¿œ |
| **ä¾å­˜é–¢ä¿‚** | é™å®šçš„ | âœ… ä»»æ„ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª |
| **ä¸¦åˆ—å‡¦ç†** | åˆ¶é™ã‚ã‚Š | âœ… è‡ªç”±ãªåˆ¶å¾¡ |

### 5.2 è¿½åŠ ã•ã‚Œã‚‹ãƒ¡ãƒªãƒƒãƒˆ

- **å°‚ç”¨ãƒªã‚½ãƒ¼ã‚¹**: CPU/ãƒ¡ãƒ¢ãƒªã‚’éŸ³å£°å‡¦ç†å°‚ç”¨ã«ä½¿ç”¨
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: è² è·ã«å¿œã˜ãŸè‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
- **ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°**: è©³ç´°ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
- **æŸ”è»Ÿæ€§**: éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è‡ªç”±ãªé¸æŠ

ã“ã®æ§‹æˆã«ã‚ˆã‚Šã€é«˜å“è³ªã§å®‰å®šã—ãŸéŸ³å£°å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚